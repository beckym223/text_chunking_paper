# Libraries

```{r message=FALSE, warning=FALSE}

library(tidyverse)
library(stringr)
library(stringi)

source("text_funcs.R")

```


# Load paragraph data frame, make output dir

```{r}
para_df_path<-"data_const/para_df.csv"
og_para_df<-read_csv(para_df_path,show_col_types = F)
output.dir<-"chunked_dfs"

if(!dir.exists(output.dir)){
    dir.create(output.dir, recursive = T)
}
```




# Chunking Strategy 1: Full documents

```{r}
output.name1<-"full_doc_df.csv"
doc_df <-og_para_df %>%
    group_by(doc_id)%>%
    mutate(doc_text = paste(text, collapse = '\n'))%>%
    distinct(doc_id,meeting_num,doc_text)

output.path1<-file.path(output.dir,output.name1)
write_csv(doc_df,output.path1)
cat(sprintf("\nSaved to %s",output.path1))
```

# Chunking Strategy 2: Original document pages

```{r}
page_df_path<-"data_const/page_df.csv"
page_df = read_csv(page_df_path,show_col_types = F)
out.path = file.path(output.dir,"page_df.csv")
write_csv(page_df,out.path)
```


# Chunking Strategy 2: Fixed size chunks, nearest sentence

## Chunking 2.1: 200 words, nearest sentence
```{r}
output.name2<-"sentence_200_plus_df.csv"
output.path2<-file.path(output.dir,output.name2)
sentence_df <- doc_df %>%
    mutate(sentence = get_sentences(doc_text))%>%
    unnest(sentence) %>%
    filter(str_detect(sentence,"[a-zA-Z]+"))%>%
        mutate(num_words = get_num_words(sentence))%>%
    group_by(doc_id)%>%
    mutate(sent_id = paste0(doc_id,"-s",row_number()))%>%
    select(doc_id, meeting_num, sent_id, sentence,num_words)

sentence_200_indexed<-sentence_df%>%
    group_by(doc_id)%>%
    mutate(sent_idx = combine_chunks_to_size(num_words,200,split_before_target=F))


sentence_200_final<-sentence_200_indexed%>%
    group_by(doc_id,sent_idx)%>%
    mutate(text = paste(sentence,collapse=" "),
           text_id = paste0(doc_id,"-",sent_idx)
           )%>%
    ungroup()%>%
    distinct(doc_id,text_id, meeting_num, text)
    

write_csv(sentence_200_final,output.path2)
```


## Chunking 2.2: 500 words, nearest sentence

```{r}

output.name3<-"sentence_500_plus_df.csv"
output.path3<-file.path(output.dir,output.name3)
sentence_df <- doc_df %>%
    mutate(sentence = get_sentences(doc_text))%>%
    unnest(sentence) %>%
    filter(str_detect(sentence,"[a-zA-Z]+"))%>%
        mutate(num_words = get_num_words(sentence))%>%
    group_by(doc_id)%>%
    mutate(sent_id = paste0(doc_id,"-s",row_number()))%>%
    select(doc_id, meeting_num, sent_id, sentence,num_words)

sentence_500_indexed<-sentence_df%>%
    group_by(doc_id)%>%
    mutate(sent_idx = combine_chunks_to_size(num_words,500,split_before_target=F))


sentence_500_final<-sentence_500_indexed%>%
    group_by(doc_id,sent_idx)%>%
    mutate(text = paste(sentence,collapse=" "),
           text_id = paste0(doc_id,"-",sent_idx)
           )%>%
    ungroup()%>%
    distinct(doc_id,text_id, meeting_num, text)
    

write_csv(sentence_500_final,output.path3)
```

# Chunking strategy 3: Author-created paragraphs
```{r}

out.path = file.path(output.dir, "para_df.csv")
write_csv(og_para_df,out.path)

```

